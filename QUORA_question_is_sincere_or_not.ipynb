{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QUORA_question_is_sincere_or_not.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushkubb/nlp/blob/main/QUORA_question_is_sincere_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5NODNZTLZ5"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1qyIqgII4yWXEkTC03x_sNbPeZa6MRWRJ\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtniugtmL7Hq"
      },
      "source": [
        "Problem Definition\n",
        "---\n",
        "I think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( Problem Formalization)\n",
        "\n",
        "> We will be predicting whether a question asked on Quora is sincere or not\n",
        "\n",
        "Data Source : https://www.kaggle.com/c/quora-insincere-questions-classification/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EetpkIFrL7Hr"
      },
      "source": [
        "**About Quora**\n",
        "\n",
        "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWgdhbAyL7Hs"
      },
      "source": [
        "**Business View**\n",
        "\n",
        "An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkUsIvstL7Hu"
      },
      "source": [
        "**What is a insincere question?**\n",
        "\n",
        "Is defined as a question intended to make a statement rather than look for helpful answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ibsQaX1L7Hu"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1XOvVoEDjNF_p2tHV1H9XzoZHxfzBBA5t\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8DpyA8uL7Hv"
      },
      "source": [
        "**Feature Set**\n",
        "\n",
        "We use train.csv and test.csv as Input and we should upload a submission.csv as Output.\n",
        "\n",
        "The training set contains the following 3 features (for Supervised Learning)\n",
        "1. qid - unique question identifier\n",
        "2. question_text - Quora question text\n",
        "3. target - a question labeled \"insincere\" has a value of 1, otherwise 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHTl3rm-umrj",
        "outputId": "1c381f93-1cc6-4522-8c5d-210536cab296"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6LX-ZPVL7Hw"
      },
      "source": [
        "**Coding a solutiuon for solving the above problem**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyaEFjqCL7Hx"
      },
      "source": [
        "# all import statements\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from wordcloud import WordCloud as wc   # not needed\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pylab as pylab\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import get_dummies\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import warnings\n",
        "import sklearn\n",
        "import string\n",
        "import scipy\n",
        "import numpy\n",
        "import nltk\n",
        "import json\n",
        "import sys\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiFm9TYL7H3",
        "outputId": "9039cc14-9ecb-40cb-f3e3-98f56e666eff"
      },
      "source": [
        "# printing versions of the important packages\n",
        "print('matplotlib: {}'.format(matplotlib.__version__))\n",
        "print('sklearn: {}'.format(sklearn.__version__))\n",
        "print('scipy: {}'.format(scipy.__version__))\n",
        "print('seaborn: {}'.format(sns.__version__))\n",
        "print('pandas: {}'.format(pd.__version__))\n",
        "print('numpy: {}'.format(np.__version__))\n",
        "print('Python: {}'.format(sys.version))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matplotlib: 3.2.2\n",
            "sklearn: 0.22.2.post1\n",
            "scipy: 1.4.1\n",
            "seaborn: 0.11.1\n",
            "pandas: 1.1.5\n",
            "numpy: 1.19.5\n",
            "Python: 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpNABB8zL7H9"
      },
      "source": [
        "We would first do EDA ( Exploratory Data Analysis ) over Quora Data set :\n",
        "--"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJY9XLX1L7H-"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1uvFGaW5OUQGbNphJqBIlaIg7Z2xIXrfX\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xion-N2GNkfN"
      },
      "source": [
        "# Download datasets from the below given links\n",
        "\n",
        "# QuoratrainSet.csv\n",
        "#  https://drive.google.com/open?id=1EBNVP7zikYFfCz0sW1hTT516Gqb8TnKG\n",
        "\n",
        "# Quoratestdata.csv\n",
        "#  https://drive.google.com/open?id=1Ep-bFgiHUBdT8gOpVDuYc7lsiAS6u7gE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eGexzHGu_6J"
      },
      "source": [
        "from google.colab import drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e84jMMv1vB2-",
        "outputId": "3bbde636-5b1e-4452-b882-e78d2a8dfb55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqhY3BhL7H_"
      },
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/DSTRAIN/NLP/QuoratrainSet.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/DSTRAIN/NLP/Quoratestdata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4IhgLbpL7IF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "46d2dadf-e7ac-4b63-b7ff-0d99b74f6cec"
      },
      "source": [
        "# check top 5 records of training dataset\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you enco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid  ... target\n",
              "0  00002165364db923c7e6  ...      0\n",
              "1  000032939017120e6e44  ...      0\n",
              "2  0000412ca6e4628ce2cf  ...      0\n",
              "3  000042bf85aa498cd78e  ...      0\n",
              "4  0000455dfa3e01eae3af  ...      0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8jErTpwL7IK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6b2a3cae-ec2d-43b4-9ea2-ce6642cd9cd8"
      },
      "source": [
        "# check top 5 records of testing dataset\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000163e3ea7c7a74cd7</td>\n",
              "      <td>Why do so many women become so rude and arroga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00002bd4fb5d505b9161</td>\n",
              "      <td>When should I apply for RV college of engineer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00007756b4a147d2b0b3</td>\n",
              "      <td>What is it really like to be a nurse practitio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000086e4b7e1c7146103</td>\n",
              "      <td>Who are entrepreneurs?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000c4c3fbe8785a3090</td>\n",
              "      <td>Is education really making good people nowadays?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid                                      question_text\n",
              "0  0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...\n",
              "1  00002bd4fb5d505b9161  When should I apply for RV college of engineer...\n",
              "2  00007756b4a147d2b0b3  What is it really like to be a nurse practitio...\n",
              "3  000086e4b7e1c7146103                             Who are entrepreneurs?\n",
              "4  0000c4c3fbe8785a3090   Is education really making good people nowadays?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvqzxXS3L7IP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7384b1-f871-4040-ffb9-c332e5fa711c"
      },
      "source": [
        "# Find the type of features in Quora dataset\n",
        "# i.e get a quick statistics\n",
        "train.dtypes\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qid              object\n",
              "question_text    object\n",
              "target            int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YolH_-8DL7IT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b756ef33-81f2-4a18-b526-5c636a6d93a1"
      },
      "source": [
        "print(test.info())  # see carefully the last value is -> None. \n",
        "                    # indicating that there are no \"Null\" values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 375806 entries, 0 to 375805\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   qid            375806 non-null  object\n",
            " 1   question_text  375806 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 5.7+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqqjg4iNL7IX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1516ddd3-d4f2-4d0d-bba3-9911aede1061"
      },
      "source": [
        "# shape for train and test\n",
        "print('Shape of train:',train.shape)\n",
        "print('Shape of test:',test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train: (1048575, 3)\n",
            "Shape of test: (375806, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPq2xmWkL7Ib"
      },
      "source": [
        "**Data Cleaning**\n",
        "\n",
        "When dealing with real-world data, dirty data is the norm rather than the exception. \n",
        "\n",
        "The primary goal of data cleaning is to detect and remove errors and anomalies to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waphM_ttL7Ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4b3aa4-2638-4e92-cc96-f55c8fc750eb"
      },
      "source": [
        "# How many NA elements in every column!!\n",
        "# Good news, it is Zero!\n",
        "# To check out how many null info are on the dataset, we can use isnull().sum().\n",
        "# recall from info() -> we found that it has zero Nulls. \n",
        "\n",
        "train.isnull().sum()\n",
        "\n",
        "# data is infact clean and ready for use."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qid              0\n",
              "question_text    0\n",
              "target           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Bm0Ys2L7Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4958f20b-1679-44e6-a9ee-9e660c56eea7"
      },
      "source": [
        "# in case , their were NA or None values in any row then we would drop the row.\n",
        "\n",
        "# remove rows that have NA's\n",
        "print('Before Droping',train.shape)\n",
        "train = train.dropna()\n",
        "print('After Droping',train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Droping (1048575, 3)\n",
            "After Droping (1048575, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5T1QBvjL7Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4e4aba-8b9c-40cb-ae43-be6ee3b1559b"
      },
      "source": [
        "# Number of words in the text\n",
        "\n",
        "train[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
        "print('maximum of num_words in train',train[\"num_words\"].max())\n",
        "#print('min of num_words in train',train[\"num_words\"].min())\n",
        "print(\"maximum of  num_words in test\",test[\"num_words\"].max())\n",
        "#print('min of num_words in train',test[\"num_words\"].min())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum of num_words in train 134\n",
            "maximum of  num_words in test 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoTMnSdTL7Ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73d692b-f77e-4353-dedd-7bb5c0f0fc70"
      },
      "source": [
        "# Number of unique words in the text\n",
        "train[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "print('maximum of num_unique_words in train',train[\"num_unique_words\"].max())\n",
        "\n",
        "print(\"maximum of num_unique_words in test\",test[\"num_unique_words\"].max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum of num_unique_words in train 96\n",
            "maximum of num_unique_words in test 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggCNxifdL7It",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1636377-4947-4bdb-e630-2bd43aa0560d"
      },
      "source": [
        "# Number of stopwords in the text\n",
        "\n",
        "#from nltk.corpus import stopwords\n",
        "eng_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "train[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "\n",
        "print('maximum of num_stopwords in train',train[\"num_stopwords\"].max())\n",
        "print(\"maximum of num_stopwords in test\",test[\"num_stopwords\"].max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a132a0a3c292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from nltk.corpus import stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0meng_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_stopwords\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meng_stopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM1Nc6NUL7Ix"
      },
      "source": [
        "# Number of punctuations in the text\n",
        "\n",
        "train[\"num_punctuations\"] =train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "print('maximum of num_punctuations in train',train[\"num_punctuations\"].max())\n",
        "print(\"maximum of num_punctuations in test\",test[\"num_punctuations\"].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MZtXmbrL7I1"
      },
      "source": [
        "# lets figure out how many unique target values exist.\n",
        "# like we expect : 0 -> sincere qns and 1 -> in-sincere qns\n",
        "\n",
        "# You see number of unique item for Target with command below:\n",
        "train_target = train['target'].values\n",
        "\n",
        "np.unique(train_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2QRVV9L7I5"
      },
      "source": [
        "**YES, quora problem is a binary classification!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXdrVRVaL7I7"
      },
      "source": [
        "**The data is absolutely clean. But is it balanced ?**\n",
        "\n",
        "I mean do we have equal no. of sincere and un-sincere questions in the dataset.\n",
        "\n",
        "lets find out :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-yJjH-AL7I8"
      },
      "source": [
        "# check no. of insincere Qns \n",
        "train.where(train ['target']==1).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2oNqJbL7I_"
      },
      "source": [
        "**This means out of ____ records __ are unsincere**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jMxYbNPL7JA"
      },
      "source": [
        "print(\"% of un-sincere qns is \", 86/1500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGU0qV8fL7JD"
      },
      "source": [
        "This means we have approx. 94% of the sincere Qns.\n",
        "\n",
        "**Such a dataset is imbalanced.**\n",
        "\n",
        ">Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "> Imbalance means that the number of data points available for different classes is different: If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M0JOOufL7JE"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1_vLZmDjxkdhTVgGm7AQedcPD2D6XnQKJ\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bq7SNJiL7JF"
      },
      "source": [
        "A typical example of imbalanced data is encountered in e-mail classification problem where emails are classified into ham or spam. The number of spam emails is usually lower than the number of relevant (ham) emails. So, using the original distribution of two classes leads to imbalanced dataset.\n",
        "\n",
        "Using accuracy as a performace measure for highly imbalanced datasets is not a good idea. For example, if 90% points belong to the true class in a binary classification problem, a default prediction of true for all data points leads to a classifier which is 90% accurate, even though the classifier has not learnt anything about the classification problem at hand!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-EdQQTfL7JF"
      },
      "source": [
        "# visualising the imbalance in data set\n",
        "\n",
        "ax=sns.countplot(x='target',hue=\"target\", data=train  ,linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n",
        "plt.title('Is data set imbalance?');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLEd1pDEL7JJ"
      },
      "source": [
        "Data Preprocessing\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyHsKxwUL7JJ"
      },
      "source": [
        "This basically involves transforming raw data into an understandable format for NLP models.\n",
        "\n",
        "Remember, our feature \"Question_Text\" is Text or String Object and No ML algo say KNN or Bayes classification would accept Text. Hence Pre-Processing is mandatory in this case.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h0Wjs8_L7JK"
      },
      "source": [
        "Below, I have recalled the two most important techniques that are also performed besides other easy to understand steps in data pre-processing:\n",
        "\n",
        "1. Tokenization: This is a process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing. NLTK Library has word_tokenize and sent_tokenize to easily break a stream of text into a list of words or sentences, respectively.\n",
        "\n",
        "\n",
        "2. Word Stemming/Lemmatization: The aim of both processes is the same, reducing the inflectional forms of each word into a common base or root. Lemmatization is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_awJz4L7JL"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1t6UR1rmEeLaVuscqA6SZTFMb3nCzYleh\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NECqbMC3L7JL"
      },
      "source": [
        "**We would atleast the following (Text) Pre-Processing steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTzKAoEeL7JM"
      },
      "source": [
        "1. Change all the text to lower case\n",
        "\n",
        "2. Word Tokenization\n",
        "\n",
        "3. Remove Stop words\n",
        "\n",
        "4. Remove Non-alpha text\n",
        "\n",
        "5. Word Lemmatization\n",
        "\n",
        "6. Converting the text data into Numeric vectors( called Vectorization )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTeankNML7JN"
      },
      "source": [
        "# step 1: Change all the text to lower case. \n",
        "\n",
        "# This is required as python interprets 'quora' and 'QUORA' differently\n",
        "\n",
        "train['question_text'] = [entry.lower() for entry in train['question_text']]\n",
        "\n",
        "test['question_text'] = [entry.lower() for entry in test['question_text']]\n",
        "\n",
        "#test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zqzSArgL7JQ"
      },
      "source": [
        "# more imports for NLP\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov_uHU84x5JO"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHreKoeVL7JU"
      },
      "source": [
        "# step 2 : Tokenization : In this each entry in the corpus will be broken \n",
        "#                         into set of words\n",
        "\n",
        "train['question_text']= [word_tokenize(entry) for entry in train['question_text']]\n",
        "\n",
        "test['question_text']= [word_tokenize(entry) for entry in test['question_text']]\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EXlcPR8L7Ja"
      },
      "source": [
        "# Set random seed\n",
        "# This is used to reproduce the same result every time \n",
        "# if the script is kept consistent otherwise each run \n",
        "# will produce different results. The seed can be set to any number.\n",
        "np.random.seed(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WotgrLE4umrx"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVNNUN7d3SIo"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLOL3tuK3kVP"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-pewMKlL7Jg"
      },
      "source": [
        "# step 3, 4 and 5\n",
        "# Remove Stop words and Numeric data \n",
        "# and perfom Word Stemming/Lemmenting.\n",
        "\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb\n",
        "# or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "# the tag_map would map any tag to 'N' (Noun) except\n",
        "# Adjective to J, Verb -> v, Adverb -> R\n",
        "# that means if you get a Pronoun then it would still be mapped to Noun\n",
        "\n",
        "\n",
        "for index,entry in enumerate(train['question_text']):\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\n",
        "    Final_words = []\n",
        "    \n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    \n",
        "    # pos_tag function below will provide the 'tag' \n",
        "    # i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(entry):\n",
        "        # Below condition is to check for Stop words and consider only \n",
        "        # alphabets\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "            \n",
        "    # The final processed set of words for each iteration will be stored \n",
        "    # in 'question_text_final'\n",
        "    train.loc[index,'question_text_final'] = str(Final_words)  \n",
        "    \n",
        "print(train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4GTDCOdL7Jj"
      },
      "source": [
        "# step 3, 4 and 5\n",
        "# Remove Stop words and Numeric data \n",
        "# and perfom Word Stemming/Lemmenting.\n",
        "\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb\n",
        "# or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "# the tag_map would map any tag to 'N' (Noun) except\n",
        "# Adjective to J, Verb -> v, Adverb -> R\n",
        "# that means if you get a Pronoun then it would still be mapped to Noun\n",
        "\n",
        "\n",
        "for index,entry in enumerate(test['question_text']):\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\n",
        "    Final_words_test = []\n",
        "    \n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    \n",
        "    # pos_tag function below will provide the 'tag' \n",
        "    # i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(entry):\n",
        "        # Below condition is to check for Stop words and consider only \n",
        "        # alphabets\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words_test.append(word_Final)\n",
        "            \n",
        "    # The final processed set of words for each iteration will be stored \n",
        "    # in 'question_text_final'\n",
        "    test.loc[index,'question_text_final'] = str(Final_words_test)    \n",
        "\n",
        "print(test.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ABQOkjKL7Jm"
      },
      "source": [
        "Tfidf_vect = TfidfVectorizer()\n",
        "Tfidf_vect.fit(train['question_text_final'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(train['question_text_final'])\n",
        "\n",
        "Test_X_Tfidf = Tfidf_vect.transform(test['question_text_final'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7WM4ftlL7Jp"
      },
      "source": [
        "# You can use the below syntax to see the vocabulary that \n",
        "# it has learned from the corpus\n",
        "print(Tfidf_vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpjw65uHL7Ju"
      },
      "source": [
        "print(Train_X_Tfidf)\n",
        "\n",
        "# Output: \n",
        "# 1: Row number of Train_X_Tfidf, \n",
        "# 2: Unique Integer number of each word, \n",
        "# 3: Score calculated by TF-IDF Vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHzt5g2L7Jx"
      },
      "source": [
        "print(Test_X_Tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_q2wZyLL7J2"
      },
      "source": [
        "Data Pre-processing is over !!\n",
        "---\n",
        "\n",
        "Use ML Algorithms to Predict the outcome\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeSFFnfuL7J3"
      },
      "source": [
        "# fit the training dataset on the MultinomialNB classifier\n",
        "Naive=naive_bayes.MultinomialNB()\n",
        "\n",
        "train_Y=train['target']\n",
        "Naive.fit(Train_X_Tfidf,train_Y)\n",
        "\n",
        "#predict the labels on validation dataset\n",
        "predictions_NB=Naive.predict(Test_X_Tfidf)\n",
        "\n",
        "print(predictions_NB)\n",
        "\n",
        "print(numpy.unique(predictions_NB,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbGs0emDWhOv"
      },
      "source": [
        "# to find the insincrere Qns according to our MultinominalNB model\n",
        "test['predictionNB']=predictions_NB\n",
        "\n",
        "QnsNB=test.loc[test['predictionNB']==1]\n",
        "\n",
        "print(QnsNB.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MneZDAvFWkHx"
      },
      "source": [
        "# just checking the MultinomialNB accuracy - approx.\n",
        "\n",
        "# predict the labels on first 5000 records of the train dataset\n",
        "testing_NB=Naive.predict(Train_X_Tfidf[:5000])\n",
        "labels_NB=train.target[:5000]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(testing_NB,labels_NB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuauCz3uumrz"
      },
      "source": [
        "#check for other metrics as the data is imbalanced\n",
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.classification_report(testing_NB, labels_NB))\n",
        "print(metrics.confusion_matrix(testing_NB,labels_NB))\n",
        "print(metrics.precision_score(testing_NB,labels_NB))\n",
        "print(metrics.recall_score(testing_NB,labels_NB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_vPzkVCumrz"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbR-SftXL7J6"
      },
      "source": [
        "# Use another Classifier Algorithm say SVM\n",
        "# fit the training dataset on the classifier\n",
        "\n",
        "SVM=svm.SVC(C=1.0,kernel='linear',degree=3,gamma='auto')\n",
        "\n",
        "SVM.fit(Train_X_Tfidf,train['target'])\n",
        "\n",
        "#predict the labels on validation dataset\n",
        "predictions_SVM=SVM.predict(Test_X_Tfidf)\n",
        "\n",
        "print(predictions_SVM)\n",
        "\n",
        "print(numpy.unique(predictions_SVM,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXJPfOnXWwpK"
      },
      "source": [
        "# to find the insincrere Qns according to our model\n",
        "test['predictions']=predictions_SVM\n",
        "\n",
        "QnsSVM=test.loc[test['predictions']==1]\n",
        "\n",
        "print(QnsSVM.question_text.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlbeh4-fWx-x"
      },
      "source": [
        "# just checking the SVM accuracy - approx.\n",
        "# predict the labels on first 5000 records of the train dataset\n",
        "\n",
        "testing_SVM=SVM.predict(Train_X_Tfidf[:5000])\n",
        "labels=train.target[:5000]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(testing_SVM,labels))\n",
        "\n",
        "#check for other metrics as the data is imbalanced\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(testing_SVM, labels))\n",
        "print(metrics.confusion_matrix(testing_SVM,labels))\n",
        "print(metrics.precision_score(testing_SVM,labels))\n",
        "print(metrics.recall_score(testing_SVM,labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBQvqtiqumr0"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AHg11T7umr0"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier()\n",
        "\n",
        "rf.fit(Train_X_Tfidf,train['target'])\n",
        "\n",
        "#predict the labels on validation dataset\n",
        "predictions_rf=rf.predict(Test_X_Tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UocW9Syvumr1"
      },
      "source": [
        "# to find the insincrere Qns according to our model\n",
        "test['predictions_rf']=predictions_rf\n",
        "\n",
        "QnsRF=test.loc[test['predictions_rf']==1]\n",
        "\n",
        "print(QnsRF.question_text.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XkHCqOumr1"
      },
      "source": [
        "# just checking the random accuracy - approx.\n",
        "# predict the labels on first 5000 records of the train dataset\n",
        "\n",
        "testing_rf=rf.predict(Train_X_Tfidf[:5000])\n",
        "labels_rf=train.target[:5000]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(testing_rf,labels_rf))\n",
        "\n",
        "#check for other metrics as the data is imbalanced\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(testing_rf, labels_rf))\n",
        "print(metrics.confusion_matrix(testing_rf,labels_rf))\n",
        "print(metrics.precision_score(testing_rf,labels_rf))\n",
        "print(metrics.recall_score(testing_rf,labels_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBfBH5vHumr1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}