{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing -Vectors- Advance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushkubb/nlp/blob/main/Text_Preprocessing_Vectors_Advance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYdy1Sx-32jP"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=11WfnSPn79Opv2rwTxldDYcv4Dv0pV-f3' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQA0QtJ6aOgZ"
      },
      "source": [
        "Implementing Word Embeddings\n",
        "--\n",
        "This section assumes that you have a working knowledge of how a neural\n",
        "network works and you can terms like :\n",
        "\n",
        "a. Deep learning\n",
        "\n",
        "b. Perceptron and Sigmoid\n",
        "\n",
        "c. FFNN ( feed forward Neural Network)\n",
        "\n",
        "d. RNN (Recurrent Neural Network )\n",
        "\n",
        "**( If new to a Neural Network (NN), it is suggested that you go through Chapter 1 to gain a basic understanding of how NN works. )\n",
        "\n",
        "Even though all previous methods solve most of the problems, once we get into more complicated problems where we want to capture the semantic relation between the words, these methods fail to perform.\n",
        "\n",
        "Below are the challenges:\n",
        "\n",
        "• All these techniques fail to capture the context and meaning of the words. All the methods discussed so far basically depend on the appearance or frequency of the words. But we need to look at how to capture the context or semantic relations: that is, how frequently the words are appearing close by.\n",
        "\n",
        ">a. I am eating an apple.\n",
        "\n",
        ">b. I am using apple.\n",
        "\n",
        "If you observe the above example, Apple gives different meanings when it is used with different (close by) adjacent words, eating and using.\n",
        "\n",
        "• For a problem like a document classification (book classification in the library), a document is really huge and there are a humongous number of tokens\n",
        "generated. In these scenarios, your number of features can get out of control (wherein) thus hampering the accuracy and performance.\n",
        "\n",
        "A machine/algorithm can match two documents/texts and say whether they are same or not. But how do we make machines tell you about cricket or Virat Kohli when you search for MS Dhoni? How do you make a machine understand that “Apple” in “Apple is a tasty fruit” is a fruit that can be eaten and not a company?\n",
        "\n",
        "The answer to the above questions lies in creating a representation for words that capture their meanings, semantic relationships, and the different types of contexts they are used in.\n",
        "\n",
        "> The above challenges are addressed by Word Embeddings.\n",
        "\n",
        "Word embedding is the feature learning technique where words from the vocabulary are mapped to vectors of real numbers capturing the contextual hierarchy.\n",
        "\n",
        "If you observe the below table, every word is represented with 4 numbers called vectors. Using the word embeddings technique, we are going to derive those vectors for each and every word so that we can use it in future analysis. In the below example, the dimension is 4. But we usually use a dimension greater than 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c5sLBLr3ygZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfZWOtLJaOgb"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=165llWGYsReLC4BCtyZs6ZLYeggkg1k1m\"  />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnIWSWCNaOgc"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to implement word embeddings.\n",
        "\n",
        "Solution\n",
        "--\n",
        "Word embeddings are prediction based, and they use shallow neural networks to train the model that will lead to learning the weight and using them as a vector representation.\n",
        "\n",
        "<font color='green'>word2vec</font>\n",
        "--\n",
        "**word2vec** is the deep learning Google framework to train word embeddings. It will use all the words of the whole corpus and predict\n",
        "the nearby words. It will create a vector for all the words present in the\n",
        "corpus in a way so that the context is captured. It also outperforms any\n",
        "other methodologies in the space of word similarity and word analogies.\n",
        "\n",
        "There are mainly 2 types of word2vec Model.\n",
        "\n",
        "• Skip-Gram\n",
        "\n",
        "• Continuous Bag of Words (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqmDRw1RaOgc"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ZC7kOYkuY2BGRCONWde38usTOCRJqJlR\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsJ6785laOgd"
      },
      "source": [
        "The above figure shows the architecture of the CBOW and skip-gram\n",
        "algorithms used to build word embeddings. Let us see how these models\n",
        "work in detail.\n",
        "\n",
        "Skip-Gram\n",
        "--\n",
        "The skip-gram model is used to predict the probabilities of a word given the context of word or words.\n",
        "\n",
        "Let us take a small sentence and understand how it actually works.\n",
        "Each sentence will generate a target word and context, which are the words\n",
        "nearby. The number of words to be considered around the target variable\n",
        "is called the window size. The table below shows all the possible target\n",
        "and context variables for window size 2. Window size needs to be selected\n",
        "based on data and the resources at your disposal. The larger the window\n",
        "size, the higher the computing power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwTt4-BMaOge"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=18nKDL_JAX96Zs_ILGMrcdd517GWLwrW2\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtDnwZltaOgf"
      },
      "source": [
        "Since it takes a lot of text and computing power, let us go ahead and take sample data and build a skip-gram model.\n",
        "\n",
        "As mentioned *in earlier NB's*, import the text corpus and break it into sentences. Perform **some cleaning and preprocessing** like the removal of\n",
        "punctuation and digits, and split the sentences into words or tokens, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64GLUiUEs_tG"
      },
      "source": [
        "# !pip uninstall gensim -y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T06:55:23.300430Z",
          "start_time": "2021-04-24T06:55:19.424547Z"
        },
        "id": "f5sRLVEOaOgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c25bbac-e067-4c7f-bafa-601f66aa9e7c"
      },
      "source": [
        "#import library\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9m3SC5xs_tG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738deae3-191d-4a0c-9275-67e25bd797c5"
      },
      "source": [
        "length=[]\n",
        "for i in sentences:\n",
        "    print(len(i))\n",
        "    length.append(len(i))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "7\n",
            "3\n",
            "9\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqc_S1eUs_tG"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvg9ZKSs_tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce4172b-134b-4db2-e9ad-20a952d10938"
      },
      "source": [
        "np.mean(length)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmvBCstCs_tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78362815-e236-43e9-9566-da0b2f0ca418"
      },
      "source": [
        "np.median(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLJ-X4KHs_tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7322451-4816-45a6-87e9-bf7a7dcf6bc5"
      },
      "source": [
        "np.min(length)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO-6YWyVs_tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b629e4ba-3148-4b86-d83d-4c5478513bf0"
      },
      "source": [
        "np.max(length)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC4UbcKVs_tH"
      },
      "source": [
        "### training the model\n",
        "https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suypdx2W5OZ1"
      },
      "source": [
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8FBJnpV5OZ3",
        "outputId": "c2c063da-f534-4d67-d44d-1ad3ca2ed365"
      },
      "source": [
        "# training the model\n",
        "skipgram = Word2Vec(sentences, size = 50, window = 3, min_count=1,sg = 1)\n",
        "# size=50 -> means size of vector to represent each token or word (default 100)\n",
        "# window=3 -> The maximum distance between the target word and its neighboring word.(default 5)\n",
        "# min_count=1 -> Minimium frequency count of words. \n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant. (default 5)\n",
        "# workers -> How many threads to use behind the scenes? (default 3) \n",
        "# sg -> (default 0 or CBOW) The training algorithm, either CBOW (0)     \n",
        "#                           or skip gram (1).\n",
        "# access vector for one word\n",
        "\n",
        "print(skipgram['nlp'])\n",
        "\n",
        "# Since our vector size parameter was 50, the model \n",
        "# gives a vector of size 50 for each word."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.00507417  0.00798924  0.00214572  0.00105395 -0.00132298 -0.00317645\n",
            " -0.00226661 -0.00463028  0.00154063  0.00764827  0.00364192  0.00231408\n",
            "  0.00700611 -0.0036976  -0.00864142  0.00363485 -0.00610256 -0.00287278\n",
            "  0.00247946  0.00054521  0.00839198 -0.00729082 -0.00313484 -0.00671063\n",
            "  0.00820201 -0.00541423  0.00295527  0.00052783  0.00714301 -0.00989141\n",
            " -0.00738953 -0.00322712 -0.0037096  -0.0012615  -0.00717171 -0.00835311\n",
            " -0.00822171  0.0058208  -0.00906421 -0.00531903 -0.00016989 -0.00074663\n",
            " -0.00909911  0.00099359  0.00258831  0.0093498  -0.00546927 -0.00868888\n",
            "  0.00358278 -0.00153129]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2OY-MQOaOgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "345a1815-4181-47e7-9018-9c2fce361b3d"
      },
      "source": [
        "# access vector for another one word\n",
        "print(skipgram['deep'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9fcdf829f234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# access vector for another one word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deep'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'deep' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_3UHcWz5OZ5"
      },
      "source": [
        "**Note** : We get an error saying the word doesn’t exist because this word was not there in our input training data. This is the reason we need to train the algorithm on as much data possible so that we do not miss out on words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUt8y7Fb5OZ5"
      },
      "source": [
        "Continuous Bag of Words (CBOW)\n",
        "--\n",
        "Now let’s see how to build CBOW model. (Its very similar to SkipGram model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1L9t3X85OZ6"
      },
      "source": [
        "#import library\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LKxjMEX5OZ7",
        "outputId": "eba026c1-cd31-4d6f-bc7c-aa3fde7e3dab"
      },
      "source": [
        "# training the model\n",
        "cbow = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 0)\n",
        "# size=50 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words. \n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# as sg=0 i.e no skipgram , hence default CBOW\n",
        "\n",
        "# access vector for one word\n",
        "print(cbow['nlp'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.00507417  0.00798924  0.00214572  0.00105395 -0.00132298 -0.00317645\n",
            " -0.00226661 -0.00463028  0.00154063  0.00764827  0.00364192  0.00231408\n",
            "  0.00700611 -0.0036976  -0.00864142  0.00363485 -0.00610256 -0.00287278\n",
            "  0.00247946  0.00054521  0.00839198 -0.00729082 -0.00313484 -0.00671063\n",
            "  0.00820201 -0.00541423  0.00295527  0.00052783  0.00714301 -0.00989141\n",
            " -0.00738953 -0.00322712 -0.0037096  -0.0012615  -0.00717171 -0.00835311\n",
            " -0.00822171  0.0058208  -0.00906421 -0.00531903 -0.00016989 -0.00074663\n",
            " -0.00909911  0.00099359  0.00258831  0.0093498  -0.00546927 -0.00868888\n",
            "  0.00358278 -0.00153129]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oge0K2P75OZ7"
      },
      "source": [
        "Important Observation \n",
        "--\n",
        "To train these models, it requires a huge amount of computing\n",
        "power. So, let us go ahead and use Google’s pre-trained model, which has\n",
        "been trained with over 100 billion words.\n",
        "\n",
        "Download the model from the below path and keep it in your local\n",
        "storage:\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "\n",
        "or **better off from this link** :\n",
        "\n",
        "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "Note **if running on Jupyter NB** : The Google Db is soo large that we would get ValueError, like this : ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GrsZ7_G5OZ7"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# import gensim package\n",
        "import gensim\n",
        "\n",
        "# load the saved model\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format('datasets/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S28_-M0P5OZ8",
        "outputId": "d97067bf-1814-48e3-9a9d-4b7d072dd11d"
      },
      "source": [
        "# lets check similarity\n",
        "print (model.similarity('This', 'is'))\n",
        "\n",
        "#Lets check one more.\n",
        "print (model.similarity('post', 'book'))\n",
        "\n",
        "#print(model.similarity('seed', 'need'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3030219\n",
            "0.057204384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfGZ9gA5OZ8"
      },
      "source": [
        "“`This`” and “`is`” have a good amount of similarity, but the similarity\n",
        "between the words “`post`” and “`book`” is poor. For any given set of words, it uses the vectors of both the words and calculates the similarity between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "iaxBTZbz5OZ8",
        "outputId": "257fe3e1-d35e-4507-b1cd-4d923dfde3c0"
      },
      "source": [
        "# Finding the odd one out.\n",
        "model.doesnt_match('breakfast cereal dinner lunch'.split())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cereal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG47_9it5OZ9"
      },
      "source": [
        "Of '`breakfast`’, ‘`cereal`’, ‘`dinner`’ and ‘`lunch`', only **cereal** is the word that is\n",
        "not anywhere related to the remaining 3 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chdni9rC5OZ9",
        "outputId": "111ca217-c6c7-4e14-af22-8bcb5290b78f"
      },
      "source": [
        "# It is also finding the relations between words.\n",
        "#model.most_similar(positive=['woman', 'king'] , negative=['man'])  # default value of topn is 10\n",
        "\n",
        "# try this too :\n",
        "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGQ4BtzK5OZ9"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=11Yu1Gj4Rw5BccL6KXnT_rXqYPyJbEUfZ\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZo1BoG5OZ9"
      },
      "source": [
        "# Implementing <font color='green'>fastText</font>\n",
        "--\n",
        "**fastText** is another deep learning framework developed by Facebook to capture context and meaning.\n",
        "\n",
        "Problem\n",
        "--\n",
        "How to implement fastText in Python.\n",
        "\n",
        "Solution\n",
        "--\n",
        "fastText is the improvised version of word2vec. word2vec basically\n",
        "considers words to build the representation. But fastText takes each\n",
        "character while computing the representation of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wRpOBG75OZ9",
        "outputId": "714abb04-9653-4754-e4e9-e9e9ede94601"
      },
      "source": [
        "# Let us see how to build a fastText word embedding.\n",
        "# Import FastText\n",
        "from gensim.models import FastText\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]\n",
        "\n",
        "fast = FastText(sentences,size=10, window=1, min_count=1, workers=5, min_n=1, max_n=2)\n",
        "# size=10 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words. \n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# min_n=1, max_n=2  -> When finding similarity or analogies like this :\n",
        "# \"Father\" - \"Boy\" + \"Girl\" == \"Mother\"\n",
        "#print(fast.most_similar(['girl', 'father'], ['boy'], topn=3))\n",
        "# [('mother', 0.7996115684509277), ('grandfather', 0.7629683613777161), \n",
        "# ('wife', 0.7478234767913818)]\n",
        "# we want the model to show min 1 and max 2 analogies\n",
        "\n",
        "\n",
        "# vector for word nlp\n",
        "print(fast['nlp'])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0012215   0.02347985 -0.01262391 -0.02205611 -0.00839358 -0.00156716\n",
            " -0.00200545  0.00993094 -0.00620706 -0.0087144 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgmbnqha5OZ-",
        "outputId": "72fe32a2-106b-4637-aac6-c0ca7a3117e3"
      },
      "source": [
        "# Try this \n",
        "print(fast.most_similar(['machine', 'learning'], ['nlp'], topn=3))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('saves', 0.8040894269943237), ('months', 0.6550357341766357), ('industry', 0.6398235559463501)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rommL3qVaOhM"
      },
      "source": [
        "<h3><font color='green'><b>I am sure !! </b></font> </h3>\n",
        "\n",
        "By now you are familiar and comfortable with processing the natural language. Now that data is cleaned and features are created,let’s jump into building some applications around it that solves the business problem; in the <b>upcoming NB's</b>.\n",
        "\n",
        "<font color='green'>Before Moving ahead <b>I would highly recommend</b> all watching this you tube <u><b>video</b></u> :</font> <br> https://www.youtube.com/watch?v=LSS_bos_TPI\n",
        "\n",
        "<b>This would further clarify concept of Word Embeddings.</b>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLQ7en-z5OZ-"
      },
      "source": [
        "**Just in case**\n",
        "\n",
        "You would love to explore **Stanford’s GloVe Embedding**  , very similar to above libraries :\n",
        "\n",
        "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "<hr>\n",
        "<br><br>\n",
        "<u><b>Further Resources</b></u> :\n",
        "\n",
        "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "\n",
        "https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:24:53.892239Z",
          "start_time": "2021-04-24T07:24:53.869307Z"
        },
        "id": "0YvDTHuhs_tQ"
      },
      "source": [
        "documents = [\n",
        "    \"Human machine interface, for lab abc& computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees?\",\n",
        "    \"Graph minors IV Widths of trees and well. quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86e0bx6Ks_tQ"
      },
      "source": [
        "### Activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:24:55.408505Z",
          "start_time": "2021-04-24T07:24:55.394006Z"
        },
        "id": "xZj6PoQos_tR"
      },
      "source": [
        "# cleaning the texts\n",
        "\n",
        "# remove common words and tokenize\n",
        "\n",
        "# remove words that appear only once"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mubUzw2js_tR"
      },
      "source": [
        "### Activity- Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:24:56.183834Z",
          "start_time": "2021-04-24T07:24:56.156226Z"
        },
        "id": "vBmsaMUos_tR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca75621-d67b-4076-adb2-cf1a29118f3e"
      },
      "source": [
        "from pprint import pprint  # pretty-printer\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "#Get the character set\n",
        "characters=set()\n",
        "for sent in documents:\n",
        "    for word in sent.split():\n",
        "        for char in word:\n",
        "            characters.add(char.lower())\n",
        "            \n",
        "# cleaning the texts\n",
        "\n",
        "documents_clean=[]\n",
        "\n",
        "for sent in documents:\n",
        "#     print(sent)\n",
        "    sent=re.sub(\"&\",\"\",sent)\n",
        "    sent=re.sub(\",\",\"\",sent)\n",
        "    sent=re.sub(\"\\?\",\"\",sent)\n",
        "    sent=re.sub(\"\\.\",\"\",sent)\n",
        "#     print(sent)\n",
        "    documents_clean.append(sent)\n",
        "\n",
        "# remove common words and tokenize\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents_clean\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "pprint(texts)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'],\n",
            " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'system'],\n",
            " ['system', 'human', 'system', 'eps'],\n",
            " ['user', 'response', 'time'],\n",
            " ['trees'],\n",
            " ['graph', 'trees'],\n",
            " ['graph', 'minors', 'trees'],\n",
            " ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:26:24.732250Z",
          "start_time": "2021-04-24T07:26:24.645144Z"
        },
        "id": "EpBuG1kss_tR"
      },
      "source": [
        "skipgram = Word2Vec(texts, size = 50, window = 6, min_count=1,sg = 1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:26:26.145024Z",
          "start_time": "2021-04-24T07:26:26.128155Z"
        },
        "id": "IWTD9ltjs_tR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3688d28b-70dc-42b6-9e9b-749f2fef71b2"
      },
      "source": [
        "vector = skipgram.wv['computer']  # get numpy vector of a word\n",
        "sims = skipgram.wv.most_similar('computer', topn=10)  # get other similar words\n",
        "print(sims)\n",
        "#Try with more number of epochs"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('human', 0.21674661338329315), ('interface', 0.11780162155628204), ('survey', 0.11475444585084915), ('user', 0.10344666242599487), ('trees', 0.09261929988861084), ('graph', 0.03716857731342316), ('time', 0.028900787234306335), ('eps', 0.02266264148056507), ('system', -0.051353201270103455), ('minors', -0.0720280259847641)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-24T07:25:14.999000Z",
          "start_time": "2021-04-24T07:25:14.982449Z"
        },
        "id": "hLL5hW6Is_tR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d854c0b8-d81a-4b26-fbe7-847b6a3c9e70"
      },
      "source": [
        "vector = skipgram.wv['computer']  # get numpy vector of a word\n",
        "sims = skipgram.wv.most_similar('computer', topn=10)  # get other similar words\n",
        "print(sims)\n",
        "#Try with more number of epochs"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('human', 0.21674661338329315), ('interface', 0.11780162155628204), ('survey', 0.11475444585084915), ('user', 0.10344666242599487), ('trees', 0.09261929988861084), ('graph', 0.03716857731342316), ('time', 0.028900787234306335), ('eps', 0.02266264148056507), ('system', -0.051353201270103455), ('minors', -0.0720280259847641)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcwbHCia7cSI"
      },
      "source": [
        "# GLOVE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyW9w4mxs_tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02cdd93-9b18-4fc3-d0f3-ce1688aeb39b"
      },
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "\n",
        "pprint(list(gensim.downloader.info()['models'].keys()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300',\n",
            " 'conceptnet-numberbatch-17-06-300',\n",
            " 'word2vec-ruscorpora-300',\n",
            " 'word2vec-google-news-300',\n",
            " 'glove-wiki-gigaword-50',\n",
            " 'glove-wiki-gigaword-100',\n",
            " 'glove-wiki-gigaword-200',\n",
            " 'glove-wiki-gigaword-300',\n",
            " 'glove-twitter-25',\n",
            " 'glove-twitter-50',\n",
            " 'glove-twitter-100',\n",
            " 'glove-twitter-200',\n",
            " '__testing_word2vec-matrix-synopsis']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VntKff7Es_tX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f174b2af-cf72-4997-ea09-facca6f54d48"
      },
      "source": [
        "# Download the \"glove-twitter-25\" embeddings\n",
        "\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "# Use the downloaded vectors as usual:\n",
        "\n",
        "glove_vectors.most_similar('twitter')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.9480051398277283),\n",
              " ('tweet', 0.9403422474861145),\n",
              " ('fb', 0.9342358708381653),\n",
              " ('instagram', 0.9104823470115662),\n",
              " ('chat', 0.8964964747428894),\n",
              " ('hashtag', 0.8885936141014099),\n",
              " ('tweets', 0.8878157734870911),\n",
              " ('tl', 0.8778461813926697),\n",
              " ('link', 0.877821147441864),\n",
              " ('internet', 0.8753897547721863)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bame8pBv7y3Y",
        "outputId": "31eb10c4-6fc0-46a9-b8c0-99752275c35c"
      },
      "source": [
        "glove_vectors['twitter']"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.6952  ,  0.42694 ,  0.14433 , -0.16535 ,  1.0463  , -0.029846,\n",
              "        0.33623 ,  1.5362  , -0.58481 ,  0.50349 , -0.50595 , -0.91136 ,\n",
              "       -3.8011  , -0.8685  , -0.13552 ,  0.97055 , -0.13545 , -0.29825 ,\n",
              "       -1.2837  , -0.63245 ,  0.44748 , -0.92231 , -0.4138  ,  0.20287 ,\n",
              "       -0.33432 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88V9XH1EBHpV"
      },
      "source": [
        "\n",
        "<hr>\n",
        "<br><br>\n",
        "<u><b>Further Resources</b></u> :\n",
        "\n",
        "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "\n",
        "https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAsDZT64uf-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}